{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea22049",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lpips -q\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/Serebryakova/Desktop/utils')\n",
    "\n",
    "import torch\n",
    "from lpips import LPIPS\n",
    "from torchvision import transforms\n",
    "from munch import Munch\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from celeba import CelebADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "params = Munch()\n",
    "params.image_size = 256\n",
    "params.batch = 32\n",
    "params.domains = 40\n",
    "params.lr = 0.0002\n",
    "params.latent = 64\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(params.image_size),\n",
    "    transforms.CenterCrop(params.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "face_data = CelebADataset(\n",
    "    root_dir='/Users/Serebryakova/Desktop/celeba',\n",
    "    transform=data_transforms)\n",
    "\n",
    "loader_workers = 0 if compute_device.type == 'cuda' else 2\n",
    "pin_memory = compute_device.type == 'cuda'\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    face_data,\n",
    "    batch_size=params.batch,\n",
    "    num_workers=loader_workers,\n",
    "    pin_memory=pin_memory,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for idx, choice in enumerate(np.random.choice(len(face_data), 9)):\n",
    "    sample, _ = face_data[choice]\n",
    "    sample = (sample - sample.min()) / (sample.max() - sample.min())\n",
    "    axes[idx//3][idx%3].imshow(sample.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f991db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.core = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            torch.nn.InstanceNorm2d(channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            torch.nn.InstanceNorm2d(channels))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input + self.core(input)\n",
    "\n",
    "class ImageGenerator(torch.nn.Module):\n",
    "    def __init__(self, channels=3, attributes=40, base=64):\n",
    "        super().__init__()\n",
    "        components = [\n",
    "            torch.nn.Conv2d(channels + attributes, base, 7, padding=3),\n",
    "            torch.nn.InstanceNorm2d(base),\n",
    "            torch.nn.ReLU()]\n",
    "        \n",
    "        current = base\n",
    "        for _ in range(2):\n",
    "            components.append(torch.nn.Conv2d(current, current*2, 4, 2, 1))\n",
    "            components.append(torch.nn.InstanceNorm2d(current*2))\n",
    "            components.append(torch.nn.ReLU())\n",
    "            current *= 2\n",
    "        \n",
    "        for _ in range(6):\n",
    "            components.append(ResidualLayer(current))\n",
    "        \n",
    "        for _ in range(2):\n",
    "            components.append(torch.nn.ConvTranspose2d(current, current//2, 4, 2, 1))\n",
    "            components.append(torch.nn.InstanceNorm2d(current//2))\n",
    "            components.append(torch.nn.ReLU())\n",
    "            current //= 2\n",
    "        \n",
    "        components.extend([\n",
    "            torch.nn.Conv2d(current, channels, 7, padding=3),\n",
    "            torch.nn.Tanh()])\n",
    "        self.layers = torch.nn.Sequential(*components)\n",
    "    \n",
    "    def forward(self, x, a):\n",
    "        a = a.view(a.size(0), -1, 1, 1).expand(-1, -1, x.shape[2], x.shape[3])\n",
    "        combined = torch.cat([x, a], 1)\n",
    "        return self.layers(combined)\n",
    "\n",
    "class ImageDiscriminator(torch.nn.Module):\n",
    "    def __init__(self, channels=3, attributes=40, base=64):\n",
    "        super().__init__()\n",
    "        modules = [\n",
    "            torch.nn.Conv2d(channels, base, 4, 2, 1),\n",
    "            torch.nn.LeakyReLU(0.01)]\n",
    "        \n",
    "        current = base\n",
    "        for _ in range(5):\n",
    "            modules.append(torch.nn.Conv2d(current, current*2, 4, 2, 1))\n",
    "            modules.append(torch.nn.LeakyReLU(0.01))\n",
    "            current *= 2\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*modules)\n",
    "        self.validator = torch.nn.Conv2d(current, 1, 3, 1, 1)\n",
    "        self.classifier = torch.nn.Conv2d(current, attributes, 3, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        validity = self.validator(features).flatten(1)\n",
    "        domain = self.classifier(features).mean([2,3])\n",
    "        return validity, domain\n",
    "\n",
    "models = Munch()\n",
    "models.generator = ImageGenerator().to(compute_device)\n",
    "models.discriminator = ImageDiscriminator().to(compute_device)\n",
    "\n",
    "adv_loss = torch.nn.MSELoss()\n",
    "cls_loss = torch.nn.BCEWithLogitsLoss()\n",
    "rec_loss = torch.nn.L1Loss()\n",
    "\n",
    "gen_optim = torch.optim.Adam(models.generator.parameters(), lr=params.lr, betas=(0.5, 0.999))\n",
    "disc_optim = torch.optim.Adam(models.discriminator.parameters(), lr=params.lr, betas=(0.5, 0.999))\n",
    "\n",
    "for epoch in range(5):\n",
    "    for batch, labels in data_loader:\n",
    "        batch, labels = batch.to(compute_device), labels.float().to(compute_device)\n",
    "        shuffled = labels[torch.randperm(labels.size(0))]\n",
    "        \n",
    "        disc_optim.zero_grad()\n",
    "        real_valid, real_cls = models.discriminator(batch)\n",
    "        loss_real = adv_loss(real_valid, torch.ones_like(real_valid))\n",
    "        loss_cls = cls_loss(real_cls, labels)\n",
    "        \n",
    "        generated = models.generator(batch, shuffled)\n",
    "        fake_valid, fake_cls = models.discriminator(generated.detach())\n",
    "        loss_fake = adv_loss(fake_valid, torch.zeros_like(fake_valid))\n",
    "        \n",
    "        total_disc = loss_real + loss_fake + loss_cls\n",
    "        total_disc.backward()\n",
    "        disc_optim.step()\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        fake_valid, fake_cls = models.discriminator(generated)\n",
    "        loss_gen = adv_loss(fake_valid, torch.ones_like(fake_valid))\n",
    "        loss_cls_gen = cls_loss(fake_cls, shuffled)\n",
    "        \n",
    "        reconstructed = models.generator(generated, labels)\n",
    "        loss_rec = rec_loss(reconstructed, batch)\n",
    "        \n",
    "        total_gen = loss_gen + loss_cls_gen + 10 * loss_rec\n",
    "        total_gen.backward()\n",
    "        gen_optim.step()\n",
    "\n",
    "perceptual_loss = LPIPS()\n",
    "\n",
    "test_results = []\n",
    "for _ in trange(100):\n",
    "    origin, _ = next(iter(data_loader))\n",
    "    reference, _ = next(iter(data_loader))\n",
    "    \n",
    "    batch_size = origin.size(0)\n",
    "    target_domain = torch.randint(params.domains, (batch_size,)).to(compute_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(batch_size, params.latent, device=compute_device)\n",
    "        style = models.mapping_network(noise, target_domain)\n",
    "        synthetic = models.generator(origin, style)\n",
    "    \n",
    "    test_results.append(perceptual_loss(synthetic.cpu(), origin.cpu()).squeeze().item())\n",
    "\n",
    "print(\"Average LPIPS:\", np.mean(test_results))\n",
    "assert np.mean(test_results) < 1.3\n",
    "\n",
    "with torch.no_grad():\n",
    "    random_noise = torch.randn(params.batch, params.latent).to(compute_device)\n",
    "    target_domain = torch.randint(params.domains, (params.batch,)).to(compute_device)\n",
    "    style = models.mapping_network(random_noise, target_domain)\n",
    "    result = models.generator(origin.to(compute_device), style)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(result[1].permute(1,2,0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153387d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_morph(image, start_attr, end_attr, steps=7):\n",
    "    repeated = image.unsqueeze(0).repeat(steps,1,1,1).to(compute_device)\n",
    "    alpha = torch.linspace(0, 1, steps).view(-1,1).to(compute_device)\n",
    "    mixed = start_attr * (1 - alpha) + end_attr * alpha\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = models.generator(repeated, mixed)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, steps, figsize=(20,8))\n",
    "    for i in range(steps):\n",
    "        axs[i].imshow(outputs[i].permute(1,2,0).cpu().numpy().clip(0,1))\n",
    "        axs[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "sample_image, sample_attr = face_data[0]\n",
    "attr_a = sample_attr.clone()\n",
    "attr_b = sample_attr.clone()\n",
    "attr_b[0] = 1 - attr_b[0]\n",
    "attribute_morph(sample_image, attr_a, attr_b, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
